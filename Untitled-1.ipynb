{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/robsvel/projetos/dockercompose/glue/Untitled-1.ipynb Célula 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/robsvel/projetos/dockercompose/glue/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboto3\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robsvel/projetos/dockercompose/glue/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Substitua pelos seus valores\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/robsvel/projetos/dockercompose/glue/Untitled-1.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m database_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdatabase_robson\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Substitua pelos seus valores\n",
    "database_name = \"database_robson\"\n",
    "table_name = \"tabela_robson\"\n",
    "bucket_name = \"bucket-robson\"\n",
    "s3_path = f\"s3://{bucket_name}/databases/\"\n",
    "\n",
    "# Substitua pela região desejada (por exemplo, \"us-east-1\")\n",
    "region = \"us-east-1\"\n",
    "\n",
    "# Inicialize o cliente S3 com a região especificada\n",
    "s3 = boto3.client('s3', region_name=region)\n",
    "\n",
    "# Verifique se o bucket já existe\n",
    "try:\n",
    "    s3.head_bucket(Bucket=bucket_name)\n",
    "    print(f'O bucket {bucket_name} já existe.')\n",
    "except Exception as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(f'O bucket {bucket_name} não existe e será criado.')\n",
    "        try:\n",
    "            s3.create_bucket(Bucket=bucket_name)\n",
    "            print(f'O bucket {bucket_name} foi criado na região {region}.')\n",
    "        except Exception as e:\n",
    "            print(f'Ocorreu um erro ao criar o bucket: {e}')\n",
    "    else:\n",
    "        print(f'Ocorreu um erro ao verificar o bucket: {e}')\n",
    "\n",
    "# Se o bucket não existir, crie-o\n",
    "\n",
    "\n",
    "# Inicialize o cliente AWS Glue\n",
    "glue = boto3.client('glue')\n",
    "\n",
    "# Verifique se o banco de dados já existe\n",
    "try:\n",
    "    response = glue.get_database(Name=database_name)\n",
    "    print(f'O banco de dados {database_name} já existe.')\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print(f'O banco de dados {database_name} não existe.')\n",
    "    # Crie o banco de dados\n",
    "    glue.create_database(DatabaseInput={'Name': database_name})\n",
    "except Exception as e:\n",
    "    print(f'Ocorreu um erro ao verificar o banco de dados: {e}')\n",
    "    \n",
    "\n",
    "# Defina o esquema da tabela\n",
    "table_input = {\n",
    "    'DatabaseName': database_name,\n",
    "    'TableInput': {\n",
    "        'Name': table_name,\n",
    "        'StorageDescriptor': {\n",
    "            'Columns': [\n",
    "                {'Name': 'Name', 'Type': 'string'},\n",
    "                {'Name': 'Age', 'Type': 'int'}\n",
    "            ],\n",
    "            'Location': s3_path,\n",
    "            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "            'Compressed': False,\n",
    "            'NumberOfBuckets': -1,\n",
    "            'SerdeInfo': {\n",
    "                'Name': table_name,\n",
    "                'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',\n",
    "                'Parameters': {}\n",
    "            }\n",
    "        },\n",
    "        'TableType': 'EXTERNAL_TABLE'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Verifique se a tabela já existe\n",
    "try:\n",
    "    response = glue.get_table(DatabaseName=database_name, Name=table_name)\n",
    "    print(f'A tabela {table_name} no banco de dados {database_name} já existe.')\n",
    "except glue.exceptions.EntityNotFoundException:\n",
    "    print(f'A tabela {table_name} no banco de dados {database_name} não existe e será criado.')\n",
    "    # Crie a tabela\n",
    "    glue.create_table(**table_input)\n",
    "except Exception as e:\n",
    "    print(f'Ocorreu um erro ao verificar a tabela: {e}')\n",
    "\n",
    "# Se quiser, atualize o Data Catalog para reconhecer as novas tabelas\n",
    "glue.get_table(DatabaseName=database_name, Name=table_name)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "# Inicialize o contexto Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "glueContext = GlueContext(spark)\n",
    "\n",
    "# Defina os dados e o esquema da tabela\n",
    "data = [(\"John Doe\", 28), (\"Jane Smith\", 32), (\"Jim Brown\", 45)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Crie um DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Escreva o DataFrame como um arquivo Parquet\n",
    "df.write.mode(\"overwrite\").parquet(path=\"s3://bucket-robson/databases/\")\n",
    "\n",
    "# Registre o DataFrame como uma tabela no AWS Glue Data Catalog\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").option(\"path\", \"s3://bucket-robson/databases/\").saveAsTable(f\"{database_name}.{table_name}\")\n",
    "\n",
    "# Execute uma consulta SQL para selecionar todos os dados da tabela\n",
    "df = spark.sql(f\"SELECT * FROM {database_name}.{table_name}\")\n",
    "\n",
    "# Mostre os dados\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
